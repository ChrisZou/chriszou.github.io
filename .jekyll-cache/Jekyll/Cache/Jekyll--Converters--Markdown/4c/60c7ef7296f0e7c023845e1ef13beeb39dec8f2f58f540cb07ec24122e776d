I"âP<p>æˆ‘ä»¬çŸ¥é“ï¼Œæ·±åº¦å­¦ä¹ æœ€æ ¸å¿ƒçš„å…¶ä¸­ä¸€ä¸ªæ­¥éª¤ï¼Œå°±æ˜¯æ±‚å¯¼ï¼šæ ¹æ®å‡½æ•°ï¼ˆlinear + activation functionï¼‰æ±‚weightsç›¸å¯¹äºlossçš„å¯¼æ•°ï¼ˆè¿˜æ˜¯lossç›¸å¯¹äºweightsçš„å¯¼æ•°ï¼Ÿï¼‰ã€‚ç„¶åæ ¹æ®å¾—å‡ºçš„å¯¼æ•°ï¼Œç›¸åº”çš„ä¿®æ”¹weightsï¼Œè®©lossæœ€å°åŒ–ã€‚
å„å¤§æ·±åº¦å­¦ä¹ æ¡†æ¶Tensorflowï¼ŒKerasï¼ŒPyTorchéƒ½è‡ªå¸¦æœ‰è‡ªåŠ¨æ±‚å¯¼åŠŸèƒ½ï¼Œä¸éœ€è¦æˆ‘ä»¬æ‰‹åŠ¨ç®—ã€‚
åœ¨åˆæ­¥å­¦ä¹ PyTorchçš„æ—¶å€™ï¼Œçœ‹åˆ°PyTorchçš„è‡ªåŠ¨æ±‚å¯¼è¿‡ç¨‹æ—¶ï¼Œæ„Ÿè§‰éå¸¸çš„åˆ«æ‰­å’Œä¸ç›´è§‚ã€‚æˆ‘ä¸‹é¢ä¸¾ä¸ªä¾‹å­ï¼Œå¤§å®¶è‡ªå·±æ„Ÿå—ä¸€ä¸‹ã€‚</p>

<pre><code class="language-python">&gt;&gt;&gt; import torch
&gt;&gt;&gt;
&gt;&gt;&gt; a = torch.tensor(2.0, requires_grad=True)
&gt;&gt;&gt; b = torch.tensor(3.0, requires_grad=True)
&gt;&gt;&gt; c = a + b
&gt;&gt;&gt; d = torch.tensor(4.0, requires_grad=True)
&gt;&gt;&gt; e = c * d
&gt;&gt;&gt;
&gt;&gt;&gt; e.backward() # æ‰§è¡Œæ±‚å¯¼
&gt;&gt;&gt; a.grad  # a.grad å³å¯¼æ•° d(e)/d(a) çš„å€¼
tensor(4.)
</code></pre>

<p>è¿™é‡Œè®©äººæ„Ÿè§‰åˆ«æ‰­çš„æ˜¯ï¼Œè°ƒç”¨ <code>e.backward()</code>æ‰§è¡Œæ±‚å¯¼ï¼Œä¸ºä»€ä¹ˆä¼šæ›´æ–° <code>a</code> å¯¹è±¡çš„çŠ¶æ€<code>grad</code>ï¼Ÿå¯¹äºä¹ æƒ¯äº†OOPçš„äººæ¥è¯´ï¼Œè¿™æ˜¯éå¸¸ä¸ç›´è§‚çš„ã€‚å› ä¸ºï¼Œåœ¨OOPé‡Œé¢ï¼Œä½ è¦æ”¹å˜ä¸€ä¸ªå¯¹è±¡çš„çŠ¶æ€ï¼Œä¸€èˆ¬çš„åšæ³•æ˜¯ï¼Œå¼•ç”¨è¿™ä¸ªå¯¹è±¡æœ¬èº«ï¼Œç»™å®ƒçš„propertyæ˜¾ç¤ºçš„èµ‹å€¼ï¼ˆæ¯”å¦‚ <code>user.age = 18</code>)ï¼Œæˆ–è€…æ˜¯è°ƒç”¨è¿™ä¸ªå¯¹è±¡çš„æ–¹æ³•ï¼ˆ<code>user.setAge(18)</code>)ï¼Œè®©å®ƒçŠ¶æ€å¾—ä»¥æ”¹å˜ã€‚
è€Œè¿™é‡Œçš„åšæ³•æ˜¯ï¼Œè°ƒç”¨äº†ä¸€ä¸ªè·Ÿå®ƒï¼ˆ<code>a</code>ï¼‰æœ¬èº«çœ‹èµ·æ¥æ²¡ä»€ä¹ˆå…³ç³»çš„å¯¹è±¡ï¼ˆ<code>e</code>ï¼‰çš„æ–¹æ³•ï¼Œç»“æœæ”¹å˜äº†å®ƒçš„çŠ¶æ€ã€‚
æ¯æ¬¡å†™ä»£ç å†™åˆ°è¿™ä¸ªåœ°æ–¹çš„æ—¶å€™ï¼Œæˆ‘éƒ½è§‰å¾—å¿ƒé‡Œä¸€æƒŠã€‚å› æ­¤ï¼Œå°±ä¸€ç›´æƒ³ä¸€æ¢ç©¶ç«Ÿï¼Œçœ‹çœ‹è¿™å†…éƒ¨çš„å…³è”ç©¶ç«Ÿæ˜¯æ€ä¹ˆæ ·çš„ã€‚
æ ¹æ®ä¸Šé¢çš„ä»£ç ï¼Œæˆ‘ä»¬çŸ¥é“çš„æ˜¯ï¼Œ<code>e</code>çš„ç»“æœï¼Œæ˜¯ç”±<code>c</code>å’Œ<code>d</code>è¿ç®—å¾—åˆ°çš„ï¼Œè€Œ<code>c</code>ï¼Œåˆæ˜¯æ ¹æ®<code>a</code>å’Œ<code>b</code>ç›¸åŠ å¾—åˆ°çš„ã€‚ç°åœ¨ï¼Œæ‰§è¡Œ<code>e</code>çš„æ–¹æ³•ï¼Œæœ€ç»ˆæ”¹å˜äº†<code>a</code>çš„çŠ¶æ€ã€‚å› æ­¤ï¼Œæˆ‘ä»¬å¯ä»¥çŒœæµ‹<code>e</code>å†…éƒ¨å¯èƒ½æœ‰æŸä¸ªä¸œè¥¿ï¼Œå¼•ç”¨ç€<code>c</code>ï¼Œç„¶åå‘¢ï¼Œ<code>c</code>å†…éƒ¨åˆæœ‰äº›ä¸œè¥¿ï¼Œå¼•ç”¨ç€<code>a</code>ã€‚å› æ­¤ï¼Œåœ¨è¿è¡Œ<code>e</code>çš„<code>backward()</code>æ–¹æ³•æ—¶ï¼Œé€šè¿‡è¿™äº›å¼•ç”¨ï¼Œå…ˆæ˜¯æ”¹å˜<code>c</code>ï¼Œåœ¨æ ¹æ®<code>c</code>å†…éƒ¨çš„å¼•ç”¨ï¼Œæœ€ç»ˆæ”¹å˜äº†<code>a</code>ã€‚å¦‚æœæˆ‘ä»¬çš„çŒœæµ‹æ²¡é”™çš„è¯ï¼Œé‚£ä¹ˆè¿™äº›å¼•ç”¨å…³ç³»åˆ°åº•æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿåœ¨ä»£ç é‡Œæ˜¯æ€ä¹ˆæç°çš„å‘¢ï¼Ÿ
æƒ³è¦çŸ¥é“å…¶ä¸­åŸç†ï¼Œæœ€å…ˆæƒ³åˆ°çš„åŠæ³•ï¼Œè‡ªç„¶æ˜¯å»çœ‹æºä»£ç ã€‚
é—æ†¾çš„æ˜¯ï¼Œ<code>backward()</code>çš„å®ç°ä¸»è¦æ˜¯åœ¨C/Cppå±‚é—´åšçš„ï¼Œåœ¨Pythonå±‚é¢åšçš„äº‹æƒ…å¾ˆå°‘ï¼ŒåŸºæœ¬ä¸Šå°±æ˜¯å¯¹å‚æ•°åšäº†ä¸€ä¸‹å¤„ç†ï¼Œç„¶åè°ƒç”¨nativeå±‚é¢çš„å®ç°ã€‚å¦‚ä¸‹ï¼š</p>

<pre><code class="language-python">def backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None):
    r"""Computes the sum of gradients of given tensors w.r.t. graph leaves.
	...more comment
    """
    if grad_variables is not None:
        warnings.warn("'grad_variables' is deprecated. Use 'grad_tensors' instead.")
        if grad_tensors is None:
            grad_tensors = grad_variables
        else:
            raise RuntimeError("'grad_tensors' and 'grad_variables' (deprecated) "
                               "arguments both passed to backward(). Please only "
                               "use 'grad_tensors'.")

    tensors = (tensors,) if isinstance(tensors, torch.Tensor) else tuple(tensors)

    if grad_tensors is None:
        grad_tensors = [None] * len(tensors)
    elif isinstance(grad_tensors, torch.Tensor):
        grad_tensors = [grad_tensors]
    else:
        grad_tensors = list(grad_tensors)

    grad_tensors = _make_grads(tensors, grad_tensors)
    if retain_graph is None:
        retain_graph = create_graph

    Variable._execution_engine.run_backward(
        tensors, grad_tensors, retain_graph, create_graph,
        allow_unreachable=True)  # allow_unreachable flag
</code></pre>

<p>è¯´åˆ°Cppã€‚ã€‚ã€‚
<img src="https://image4blogs.oss-cn-shanghai.aliyuncs.com/its_my_knowlodge_blindspot.jpeg" alt="" />
ç”±äºC/Cppæ˜¯æˆ‘çš„çŸ¥è¯†ç›²åŒºï¼Œåªèƒ½é€šè¿‡ä¸€é¡¿è‡ªè¡Œçš„æ¢ç´¢æ“ä½œï¼Œæ¥äº†è§£è¿™ä¸ªæ‰§è¡Œè¿‡ç¨‹äº†ã€‚</p>

<p>æˆ‘ä»¬å…ˆçœ‹çœ‹<code>e</code>é‡Œé¢æœ‰ä»€ä¹ˆã€‚
ç”±äº<code>e</code>æ˜¯ä¸€ä¸ª<code>Tensor</code>å˜é‡ï¼Œæˆ‘ä»¬è‡ªç„¶æƒ³åˆ°å»çœ‹<code>Tensor</code>è¿™ä¸ªç±»çš„ä»£ç ï¼Œçœ‹çœ‹é‡Œé¢æœ‰å“ªäº›æˆå‘˜å˜é‡ã€‚ä¸å¹¸çš„æ˜¯ï¼ŒPythonè¯­è¨€å£°æ˜æˆå‘˜å˜é‡çš„æ–¹å¼è·ŸJavaè¿™äº›é™æ€è¯­è¨€ä¸ä¸€æ ·ï¼Œä»–ä»¬æ˜¯ç”¨åˆ°çš„æ—¶å€™ç›´æ¥ç”¨<code>self.xxx</code>éšæ—¶å£°æ˜çš„ã€‚ä¸åƒJavaè¿™æ ·ï¼Œåœ¨æŸä¸€ä¸ªåœ°æ–¹ç»Ÿä¸€å£°æ˜å¹¶åšåˆå§‹åŒ–ã€‚
å½“ç„¶ï¼Œæˆ‘ä»¬å¯ä»¥ç”¨æ­£åˆ™è¡¨è¾¾å¼ <code>self\.\w+\s+=</code> æœç´¢æ‰€æœ‰ç±»ä¼¼äº <code>self.xxx =Â </code>çš„åœ°æ–¹ï¼Œäºæ˜¯ä½ ä¼šæ‰¾åˆ°ä¸€äº›<code>data</code>, <code>requires_grad</code>, <code>_backward_hooks</code>, <code>retain_grad</code>ç­‰ç­‰ã€‚æ ¹æ®å·²æœ‰çš„çŸ¥è¯†ï¼Œè¿™äº›çœ‹èµ·æ¥éƒ½ä¸åƒã€‚çœ‹æ¥ç›¸å…³çš„æˆå‘˜å˜é‡åº”è¯¥åœ¨å…¶çˆ¶ç±»<code>TensorBase</code>é‡Œé¢ã€‚ä¸å¹¸çš„æ˜¯ï¼Œ<code>TensorBase</code>æ˜¯ç”¨C/Cpp å®ç°çš„ã€‚è¿™ã€‚ã€‚ã€‚è¿™å°±åˆæ¶‰åŠåˆ°æˆ‘çš„çŸ¥è¯†ç›²åŒºäº†ã€‚ã€‚ã€‚</p>

<p>ä¸è¿‡ï¼ŒPythonå…¶å®è¿˜æä¾›äº†å…¶ä»–çš„ä¸€äº›æ–¹å¼ï¼Œæ¥æ–¹ä¾¿æˆ‘ä»¬æŸ¥çœ‹è¿™ä¸ªå¯¹è±¡çš„å±æ€§å’ŒçŠ¶æ€ã€‚é‚£å°±æ˜¯<code>vars()</code> æ–¹æ³•å’Œ <code>dir()</code>æ–¹æ³•ã€‚ç„¶è€Œã€‚ã€‚ã€‚</p>

<pre><code class="language-python">&gt;&gt;&gt; vars(a)
{}
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt;
&gt;&gt;&gt; dir(a)
['__abs__', '__add__', '__and__', '__array__', '__array_priority__', '__array_wrap__', '__bool__', '__class__', '__deepcopy__', '__delattr__', '__delitem__', '__dict__', '__dir__', '__div__', '__doc__', '__eq__', '__float__', '__floordiv__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__iadd__', '__iand__', '__idiv__', '__ilshift__', '__imul__', '__index__', '__init__', '__init_subclass__', '__int__', '__invert__', '__ior__', '__ipow__', '__irshift__', '__isub__', '__iter__', '__itruediv__', '__ixor__', '__le__', '__len__', '__long__', '__lshift__', '__lt__', '__matmul__', '__mod__', '__module__', '__mul__', '__ne__', '__neg__', '__new__', '__nonzero__', '__or__', '__pow__', '__radd__', '__rdiv__', '__reduce__', '__reduce_ex__', '__repr__', '__reversed__', '__rfloordiv__', '__rmul__', '__rpow__', '__rshift__', '__rsub__', '__rtruediv__', '__setattr__', '__setitem__', '__setstate__', '__sizeof__', '__str__', '__sub__', '__subclasshook__', '__truediv__', '__weakref__', '__xor__', '_backward_hooks', '_base', '_cdata', '_coalesced_', '_dimI', '_dimV', '_grad', '_grad_fn', '_indices', '_make_subclass', '_nnz', '_values', '_version', 'abs', 'abs_', 'acos', 'acos_', 'add', 'add_', 'addbmm', 'addbmm_', 'addcdiv', 'addcdiv_', 'addcmul', 'addcmul_', 'addmm', 'addmm_', 'addmv', 'addmv_', 'addr', 'addr_', 'all', 'allclose', 'any', 'apply_', 'argmax', 'argmin', 'argsort', 'as_strided', 'as_strided_', 'asin', 'asin_', 'atan', 'atan2', 'atan2_', 'atan_', 'backward', 'baddbmm', 'baddbmm_', 'bernoulli', 'bernoulli_', 'bincount', 'bmm', 'btrifact', 'btrifact_with_info', 'btrisolve', 'byte', 'cauchy_', 'ceil', 'ceil_', 'char', 'cholesky', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'clone', 'coalesce', 'contiguous', 'copy_', 'cos', 'cos_', 'cosh', 'cosh_', 'cpu', 'cross', 'cuda', 'cumprod', 'cumsum', 'data', 'data_ptr', 'dense_dim', 'det', 'detach', 'detach_', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'digamma', 'digamma_', 'dim', 'dist', 'div', 'div_', 'dot', 'double', 'dtype', 'eig', 'element_size', 'eq', 'eq_', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'erfinv_', 'exp', 'exp_', 'expand', 'expand_as', 'expm1', 'expm1_', 'exponential_', 'fft', 'fill_', 'flatten', 'flip', 'float', 'floor', 'floor_', 'fmod', 'fmod_', 'frac', 'frac_', 'gather', 'ge', 'ge_', 'gels', 'geometric_', 'geqrf', 'ger', 'gesv', 'get_device', 'grad', 'grad_fn', 'gt', 'gt_', 'half', 'hardshrink', 'histc', 'ifft', 'index_add', 'index_add_', 'index_copy', 'index_copy_', 'index_fill', 'index_fill_', 'index_put', 'index_put_', 'index_select', 'indices', 'int', 'inverse', 'irfft', 'is_coalesced', 'is_complex', 'is_contiguous', 'is_cuda', 'is_distributed', 'is_floating_point', 'is_leaf', 'is_nonzero', 'is_pinned', 'is_same_size', 'is_set_to', 'is_shared', 'is_signed', 'is_sparse', 'isclose', 'item', 'kthvalue', 'layout', 'le', 'le_', 'lerp', 'lerp_', 'lgamma', 'lgamma_', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_normal_', 'log_softmax', 'logdet', 'logsumexp', 'long', 'lt', 'lt_', 'map2_', 'map_', 'masked_fill', 'masked_fill_', 'masked_scatter', 'masked_scatter_', 'masked_select', 'matmul', 'matrix_power', 'max', 'mean', 'median', 'min', 'mm', 'mode', 'mul', 'mul_', 'multinomial', 'mv', 'mvlgamma', 'mvlgamma_', 'name', 'narrow', 'narrow_copy', 'ndimension', 'ne', 'ne_', 'neg', 'neg_', 'nelement', 'new', 'new_empty', 'new_full', 'new_ones', 'new_tensor', 'new_zeros', 'nonzero', 'norm', 'normal_', 'numel', 'numpy', 'orgqr', 'ormqr', 'output_nr', 'permute', 'pin_memory', 'pinverse', 'polygamma', 'polygamma_', 'potrf', 'potri', 'potrs', 'pow', 'pow_', 'prelu', 'prod', 'pstrf', 'put_', 'qr', 'random_', 'reciprocal', 'reciprocal_', 'record_stream', 'register_hook', 'reinforce', 'relu', 'relu_', 'remainder', 'remainder_', 'renorm', 'renorm_', 'repeat', 'requires_grad', 'requires_grad_', 'reshape', 'reshape_as', 'resize', 'resize_', 'resize_as', 'resize_as_', 'retain_grad', 'rfft', 'roll', 'rot90', 'round', 'round_', 'rsqrt', 'rsqrt_', 'scatter', 'scatter_', 'scatter_add', 'scatter_add_', 'select', 'set_', 'shape', 'share_memory_', 'short', 'sigmoid', 'sigmoid_', 'sign', 'sign_', 'sin', 'sin_', 'sinh', 'sinh_', 'size', 'slogdet', 'smm', 'softmax', 'sort', 'sparse_dim', 'sparse_mask', 'sparse_resize_', 'sparse_resize_and_clear_', 'split', 'split_with_sizes', 'sqrt', 'sqrt_', 'squeeze', 'squeeze_', 'sspaddmm', 'std', 'stft', 'storage', 'storage_offset', 'storage_type', 'stride', 'sub', 'sub_', 'sum', 'svd', 'symeig', 't', 't_', 'take', 'tan', 'tan_', 'tanh', 'tanh_', 'to', 'to_dense', 'to_sparse', 'tolist', 'topk', 'trace', 'transpose', 'transpose_', 'tril', 'tril_', 'triu', 'triu_', 'trtrs', 'trunc', 'trunc_', 'type', 'type_as', 'unbind', 'unfold', 'uniform_', 'unique', 'unsqueeze', 'unsqueeze_', 'values', 'var', 'view', 'view_as', 'where', 'zero_']
&gt;&gt;&gt;
</code></pre>

<p>å¯ä»¥çœ‹åˆ°ï¼Œä½¿ç”¨<code>vars()</code>æ–¹æ³•ï¼Œè¿”å›çš„é›†åˆæ˜¯ç©ºçš„ã€‚è€Œä½¿ç”¨<code>dir()</code>ï¼Œè¿”å›çš„å´åˆå¤ªå¤šäº†ï¼Œä½ éƒ½ä¸çŸ¥é“å“ªäº›æ˜¯æœ‰ç”¨çš„å“ªäº›æ˜¯æ²¡ç”¨çš„ï¼Œå“ªäº›åˆæ˜¯æˆ‘ä»¬çœŸæ­£å…³å¿ƒçš„ã€‚
æ€ä¹ˆåŠå‘¢ï¼Ÿ
çœ‹æ¥åªèƒ½Googleäº†ã€‚ç»è¿‡ä¸€é¡¿è°ƒæŸ¥å’Œè¿çŒœå¸¦è’™ï¼Œæˆ‘å¾—å‡ºäº†ä¸€äº›ç»“è®ºã€‚ä¹Ÿä¸çŸ¥é“æ˜¯å¦æ­£ç¡®ï¼ˆå‡†ç¡®ï¼‰ï¼Œå¦‚æœæœ‰é”™è¯¯æˆ–ä¸å‡†ç¡®çš„åœ°æ–¹ï¼Œè¿˜å¸Œæœ›æœ‰å¤§ç¥ä¸åæŒ‡å‡ºã€‚</p>

<p>ä¸ºäº†è§£é‡Šä»–ä»¬ä¹‹é—´çš„å…³ç³»ï¼Œæˆ‘ä»¬å…ˆä»ä¸€ä¸ªæœ€ç®€å•çš„ä¾‹å­å¼€å§‹ã€‚</p>

<pre><code class="language-python">&gt;&gt;&gt; a = torch.tensor(2.0, requires_grad=True)
&gt;&gt;&gt; b = torch.tensor(3.0, requires_grad=True)
&gt;&gt;&gt; c = a + b
&gt;&gt;&gt;
&gt;&gt;&gt; c.backward()
&gt;&gt;&gt; a.grad
tensor(1.)
&gt;&gt;&gt; b.grad
tensor(1.)
&gt;&gt;&gt;
</code></pre>

<p>æˆ‘ä»¬çš„é—®é¢˜æ˜¯ï¼Œ<code>c</code>å’Œ<code>a</code>æ˜¯æ€ä¹ˆä¸²è”èµ·æ¥çš„ï¼Ÿä¸ºä»€ä¹ˆæ‰§è¡Œ<code>c.backward()</code>ï¼Œä¼šæ›´æ–°<code>a</code>çš„çŠ¶æ€ï¼ˆ<code>a.grad</code>çš„å€¼ï¼‰ï¼Ÿ
å…¶å®ï¼Œæˆ‘ä»¬è¦æ‰¾çš„ä¸œè¥¿ï¼Œè¿œåœ¨å¤©è¾¹ï¼Œè¿‘åœ¨çœ¼å‰ã€‚</p>

<pre><code class="language-python">&gt;&gt;&gt; c
tensor(5., grad_fn=&lt;AddBackward0&gt;)
&gt;&gt;&gt;
</code></pre>

<p>å¯ä»¥çœ‹åˆ°ï¼Œcé‡Œé¢æœ‰ä¸€ä¸ª<code>gran_fn</code>å˜é‡ã€‚è¿™ä¸ªä¸œè¥¿æ˜¯ä»€ä¹ˆå‘¢ï¼Ÿ</p>

<pre><code class="language-python">&gt;&gt;&gt; c.grad_fn
&lt;AddBackward0 object at 0x10e56d160&gt;
&gt;&gt;&gt; type(c.grad_fn)
&lt;class 'AddBackward0'&gt;
&gt;&gt;&gt;
</code></pre>

<p>å¯è§ï¼Œè¿™æ˜¯ä¸€ä¸ª<code>AddBackward0</code>è¿™ä¸ªç±»çš„å¯¹è±¡ã€‚é—æ†¾çš„æ˜¯ï¼Œè¿™ä¸ªç±»ä¹Ÿæ˜¯ç”¨Cppæ¥å†™çš„ã€‚ä¸è¿‡ï¼Œè¿™ä¸ä»£è¡¨æˆ‘ä»¬ä¸èƒ½åœ¨Pythonå±‚åšä¸€äº›ç®€å•çš„æ¢ç´¢ï¼Œçœ‹çœ‹é‡Œé¢æœ‰äº›ä»€ä¹ˆä¸œè¥¿ã€‚</p>

<pre><code class="language-python">&gt;&gt;&gt; dir(c.grad_fn)
['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_register_hook_dict', 'metadata', 'name', 'next_functions', 'register_hook', 'requires_grad']
</code></pre>

<p>é™¤å»é‚£äº›ç‰¹æ®Šæ–¹æ³•ï¼ˆä»¥<code>__</code>å¼€å¤´å’Œç»“æŸçš„ï¼‰å’Œç§æœ‰æ–¹æ³•ï¼ˆä»¥<code>_</code>å¼€å¤´çš„ï¼‰ï¼ŒèŒƒå›´ç¼©å°åˆ°<code>['metadata', 'name', 'next_functions', 'register_hook', 'requires_gradâ€™]</code> è¿™å…¶ä¸­ï¼Œçœ‹åå­—ï¼Œæœ€å¯ç–‘çš„æ˜¯è¿™ä¸ª<code>next_functions</code>ã€‚æˆ‘ä»¬çœ‹çœ‹æ˜¯ä»€ä¹ˆï¼š</p>

<pre><code class="language-python">&gt;&gt;&gt; c.grad_fn.next_functions
((&lt;AccumulateGrad object at 0x10e56d160&gt;, 0), (&lt;AccumulateGrad object at 0x1205b29b0&gt;, 0))
&gt;&gt;&gt;
</code></pre>

<p>çœ‹èµ·æ¥ï¼Œè¿™ä¸ª<code>next_functions</code> æ˜¯ä¸€ä¸ªtuple of tuple of <code>AccumulateGrad</code> and <code>int</code>ã€‚
ç»§ç»­æ¢ç´¢è¿™ä¸ª <code>AccumulateGrad</code>ã€‚</p>

<pre><code class="language-python">&gt;&gt;&gt; ag = c.grad_fn.next_functions[0][0]
&gt;&gt;&gt; dir(ag)
['__call__', '__class__', '__delattr__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '_register_hook_dict', 'metadata', 'name', 'next_functions', 'register_hook', 'requires_grad', 'variable']
</code></pre>

<p>åŒæ ·çš„ï¼Œå»æ‰é‚£äº›ç‰¹æ®Šå‡½æ•°ã€‚æˆ‘ä»¬æ„Ÿå…´è¶£çš„èŒƒå›´ç¼©å°åˆ° <code>['metadata', 'name', 'next_functions', 'register_hook', 'requires_grad', 'variable']</code> è¿™å…¶ä¸­ï¼Œé™¤äº†å‰é¢æé«˜è¿‡çš„ <code>'next_functions'</code> ä¹‹å¤–ï¼Œæˆ‘ä»¬æƒŠè®¶çš„å‘ç°ï¼Œè¿˜æœ‰ä¸€ä¸ª å« <code>variable</code> çš„å±æ€§ã€‚æˆ‘ä»¬åˆ†åˆ«éƒ½çœ‹ä¸€ä¸‹ï¼š</p>

<pre><code class="language-python">&gt;&gt;&gt; ag.next_functions
()
&gt;&gt;&gt; ag.variable
tensor(2., requires_grad=True)
&gt;&gt;&gt;
</code></pre>

<p>å¯è§ï¼Œ<code>ag1</code>çš„<code>variable</code>è¿™ä¸ªå±æ€§æ˜¯ä¸€ä¸ª</p>

<pre><code class="language-python">tensor(2., requires_grad=True)
</code></pre>

<p>è¿™ä¸ªçœ‹èµ·æ¥ä¼¼ä¹è·Ÿæˆ‘ä»¬å‰é¢å®šä¹‰çš„aæ˜¯åŒä¸€ä¸ªå•Šã€‚æ˜¯å—ï¼Ÿæˆ‘ä»¬ç¡®è®¤ä¸€ä¸‹ï¼š</p>

<pre><code class="language-python">&gt;&gt;&gt; id(a)
4842774104
&gt;&gt;&gt; id(ag.variable)
4842774104
&gt;&gt;&gt;
</code></pre>

<p>æœç„¶æ˜¯ï¼
åˆ°è¿™é‡Œï¼Œè°œåº•åŸºæœ¬ä¸Šå°±å‘¼ä¹‹æ¬²å‡ºäº†ã€‚</p>

<p>å½“æˆ‘ä»¬æ‰§è¡Œ<code>c.backward()</code>çš„æ—¶å€™ã€‚è¿™ä¸ªæ“ä½œå°†è°ƒç”¨cé‡Œé¢çš„<code>grad_fn</code>è¿™ä¸ªå±æ€§ï¼Œæ‰§è¡Œæ±‚å¯¼çš„æ“ä½œã€‚è¿™ä¸ªæ“ä½œå°†éå†<code>grad_fn</code>çš„<code>next_functions</code>ï¼Œç„¶ååˆ†åˆ«å–å‡ºé‡Œé¢çš„functionï¼ˆ<code>AccumulateGrad</code>ï¼‰ï¼Œæ‰§è¡Œæ±‚å¯¼æ“ä½œã€‚è®¡ç®—å‡ºç»“æœä»¥åï¼Œå°†ç»“æœä¿å­˜åˆ°ä»–ä»¬å¯¹åº”çš„<code>variable</code> è¿™ä¸ªå˜é‡æ‰€å¼•ç”¨çš„å¯¹è±¡ï¼ˆ<code>a</code>å’Œ<code>b</code>ï¼‰çš„ <code>grad</code>è¿™ä¸ªå±æ€§é‡Œé¢ã€‚</p>

<p>äºæ˜¯ï¼Œå½“æˆ‘ä»¬æ‰§è¡Œå®Œ<code>c.backward()</code>ä¹‹åï¼Œ<code>a</code>å’Œ<code>b</code>é‡Œé¢çš„<code>grad</code>å€¼å°±å¾—åˆ°äº†æ›´æ–°ã€‚</p>

<p>å†å›åˆ°æˆ‘ä»¬å¼€ç¯‡æåˆ°çš„ç¨å¾®å¤æ‚ç‚¹çš„ä¾‹å­ï¼š</p>

<pre><code class="language-python">&gt;&gt;&gt; import torch
&gt;&gt;&gt;
&gt;&gt;&gt; a = torch.tensor(2.0, requires_grad=True)
&gt;&gt;&gt; b = torch.tensor(3.0, requires_grad=True)
&gt;&gt;&gt; c = a + b
&gt;&gt;&gt; d = torch.tensor(4.0, requires_grad=True)
&gt;&gt;&gt; e = c * d
&gt;&gt;&gt;
&gt;&gt;&gt; e.backward()
&gt;&gt;&gt; a.grad
tensor(4.)
&gt;&gt;&gt; b.grad
tensor(4.)
&gt;&gt;&gt; c.grad
&gt;&gt;&gt; d.grad
tensor(5.)
</code></pre>

<p>ä»¥æ­¤ç±»æ¨ï¼Œ<code>e</code>åˆ°å„ä¸ªèŠ‚ç‚¹<code>a</code>ã€<code>b</code>ã€<code>c</code>ã€<code>d</code>çš„å…³è”ä¹Ÿå°±å¾ˆå®¹æ˜“ç†è§£äº†ã€‚</p>

<pre><code class="language-python">&gt;&gt;&gt; e
tensor(20., grad_fn=&lt;MulBackward0&gt;)
&gt;&gt;&gt; e.grad_fn
&lt;MulBackward0 object at 0x111cb5470&gt;
&gt;&gt;&gt; e.grad_fn.next_functions
((&lt;AddBackward0 object at 0x110501438&gt;, 0), (&lt;AccumulateGrad object at 0x111cb5fd0&gt;, 0))
</code></pre>

<p>åˆ†åˆ«æŠŠ<code>next_functions</code>ä¸­çš„functionå–å‡ºæ¥çœ‹çœ‹</p>

<pre><code class="language-python">&gt;&gt;&gt; ((f1, _), (f2, _)) = e.grad_fn.next_functions

&gt;&gt;&gt; f1
&lt;AddBackward0 object at 0x111cb5fd0&gt;
&gt;&gt;&gt; f1.variable
Traceback (most recent call last):
  File "&lt;stdin&gt;", line 1, in &lt;module&gt;
AttributeError: 'AddBackward0' object has no attribute 'variable'

&gt;&gt;&gt; c
tensor(5., grad_fn=&lt;AddBackward0&gt;)
&gt;&gt;&gt; c.grad_fn
&lt;AddBackward0 object at 0x111cb5fd0&gt;

&gt;&gt;&gt; f2
&lt;AccumulateGrad object at 0x1103ee4e0&gt;
&gt;&gt;&gt; f2.variable
tensor(4., requires_grad=True)
</code></pre>

<p>å¯è§ï¼Œ<code>e.grad_fn.next_functions</code>ä¸­çš„ç¬¬ä¸€ä¸ªfunction <code>f1</code>ï¼Œå°±æ˜¯<code>c.grad_fn</code>ã€‚</p>

<p>ä¸è¿‡ï¼Œå¦‚æœè·Ÿç€åˆšåˆšçš„æ€è·¯ï¼Œä½ ä¼šè§‰å¾—æ„å¤–çš„æ˜¯ï¼Œ<code>f1</code>æ˜¯æ²¡æœ‰<code>variable</code> å˜é‡çš„ã€‚è¿™æ˜¯å› ä¸ºï¼Œ<code>c</code>çš„ç»“æœï¼Œæ˜¯ç”±<code>a</code>å’Œ<code>b</code>ç›¸åŠ çš„å‡ºæ¥çš„ï¼Œè¿™æ ·çš„å˜é‡æ˜¯éâ€œå¶å˜é‡â€ã€‚
å¦‚æœæˆ‘ä»¬æŠŠ<code>a</code>ã€<code>b</code>ã€<code>c</code>ã€<code>d</code>ã€<code>e</code>å’Œä»–ä»¬ä¹‹é—´çš„è¿ç®—è¿‡ç¨‹ç†è§£ä¸ºä¸€æ£µæ ‘ã€‚é‚£ä¹ˆï¼Œ<code>a</code>ã€<code>b</code>ã€<code>d</code>éƒ½æ˜¯æˆ‘ä»¬è‡ªå·±â€œnewâ€å‡ºæ¥çš„ï¼Œè¿™æ ·çš„èŠ‚ç‚¹å«å¶èŠ‚ç‚¹ã€‚è¿™äº›å¶èŠ‚ç‚¹åˆ†åˆ«æœ‰ä¸€ä¸ª<code>AccumulateGrad</code> ç±»å‹çš„functionè·Ÿå®ƒä»¬å¯¹åº”èµ·æ¥ã€‚åˆ™åƒcã€eè¿™äº›ï¼Œä¸æ˜¯æˆ‘ä»¬è‡ªå·±ç›´æ¥åˆ›å»ºçš„ï¼Œè€Œæ˜¯é€šè¿‡ä¸€äº›è¿ç®—å¾—å‡ºçš„ï¼Œå°±æ˜¯éå¶èŠ‚ç‚¹ã€‚å¯¹äºéå¶èŠ‚ç‚¹æ¥è¯´ï¼Œé»˜è®¤æƒ…å†µä¸‹ä»–ä»¬ä¸éœ€è¦å­˜å‚¨å¯¼æ•°å€¼ï¼ˆå½“ç„¶ï¼Œå¦‚æœéœ€è¦ï¼Œä¹Ÿæ˜¯æœ‰åŠæ³•åšåˆ°çš„ï¼‰ã€‚å› æ­¤ï¼Œä»–ä»¬çš„<code>grad_fn</code>ï¼Œä¸éœ€è¦æœ‰ä¸€ä¸ªå˜é‡<code>variable</code> å¼•ç”¨ç€ä»–ä»¬ã€‚</p>

<p>åœ¨<code>e.backward()</code>æ‰§è¡Œæ±‚å¯¼æ—¶ï¼Œç³»ç»Ÿéå†<code>e.grad_fn.next_functions</code>ï¼Œåˆ†åˆ«æ‰§è¡Œæ±‚å¯¼ã€‚å¦‚æœ<code>e.grad_fn.next_functions</code>ä¸­æœ‰å“ªä¸ªæ˜¯<code>AccumulateGrad</code>ï¼Œåˆ™æŠŠç»“æœä¿å­˜åˆ°<code>AccumulateGrad</code>çš„variableå¼•ç”¨çš„å˜é‡ä¸­ã€‚å¦åˆ™ï¼Œé€’å½’éå†è¿™ä¸ªfunctionçš„<code>next_functions</code>ï¼Œæ‰§è¡Œæ±‚å¯¼è¿‡ç¨‹ã€‚æœ€ç»ˆåˆ°è¾¾æ‰€æœ‰çš„å¶èŠ‚ç‚¹ï¼Œæ±‚å¯¼ç»“æŸã€‚åŒæ—¶ï¼Œæ‰€æœ‰çš„å¶èŠ‚ç‚¹çš„<code>grad</code>å˜é‡éƒ½å¾—åˆ°äº†ç›¸åº”çš„æ›´æ–°ã€‚
ä»–ä»¬ä¹‹é—´çš„å…³ç³»å¦‚ä¸‹å›¾æ‰€ç¤ºï¼š
<img src="https://image4blogs.oss-cn-shanghai.aliyuncs.com/autograd_graph.png" alt="" /></p>

<p>é‚£ä¹ˆï¼Œè¿˜æœ‰ä¸¤ä¸ªé—®é¢˜æ²¡æœ‰è§£å†³ï¼š</p>
<ol>
  <li>è¿™äº›å„ç§functionï¼Œåƒ<code>AccumulateGrad</code>ã€<code>AddBackward0</code>ã€<code>MulBackward0</code>ï¼Œæ˜¯æ€ä¹ˆäº§ç”Ÿçš„ï¼Ÿ</li>
  <li>è¿™äº›functionï¼Œæ¯”å¦‚ä¸Šé¢å‡ºç°è¿‡çš„<code>AddBackward0</code> ã€<code>MulBackward0</code>ï¼Œå…·ä½“æ˜¯æ€ä¹ˆæ±‚å¯¼çš„å‘¢ï¼Ÿ</li>
</ol>

<p>å¯¹äºç¬¬ä¸€ä¸ªé—®é¢˜ï¼Œå¾ˆè‡ªç„¶çš„çŒœæµ‹ï¼Œæ˜¯PyTorché‡å†™äº†ä¸€äº›æ“ä½œç¬¦ï¼Œåƒ<code>+</code>ï¼Œ<code>*</code>ç­‰ã€‚åœ¨è¿™ä¸ªè¿‡ç¨‹ä¸­ï¼Œåˆ›å»ºäº†è¿™äº›functionï¼Œå¹¶å»ºç«‹èµ·äº†å¼•ç”¨å…³ç³»ã€‚
å¯¹äºç¬¬äºŒä¸ªé—®é¢˜ï¼Œç®€å•çš„è¯´ï¼Œå°±æ˜¯åœ¨æ¯ä¸ªå‡½æ•°å®šä¹‰çš„æ—¶å€™ï¼Œéƒ½éœ€è¦è‡ªå·±å®šä¹‰å¥½<code>forward()</code>å’Œ<code>backward()</code>å‡½æ•°ã€‚åœ¨<code>forward()</code>é‡Œé¢å®ç°è¿™ä¸ªè¿ç®—çš„æ‰§è¡Œè¿‡ç¨‹ã€‚æ¯”å¦‚ï¼Œç›¸åŠ ã€ç›¸ä¹˜ï¼Œåœ¨<code>backward()</code>åˆ™å®ç°è¿™ä¸ªè¿ç®—çš„æ±‚å¯¼è¿‡ç¨‹ã€‚</p>

<p>ä»¥ä¸Šå°±æ˜¯æˆ‘å¯¹PyTorchçš„è‡ªåŠ¨æ±‚å¯¼åŸç†çš„ç†è§£ã€‚åªæ˜¯ä¸€ä¸ªå¤§æ¦‚çš„ï¼Œæ¯”è¾ƒæµ…æ˜¾çš„ç†è§£ã€‚å¯¹äºä¸€äº›æ›´åŠ ç»†èŠ‚çš„ï¼ŒåŒ…æ‹¬ä¸€äº›ç‰¹æ®Šæƒ…å†µçš„å¤„ç†ï¼Œæ¨èå¤§å®¶çœ‹è¿™ä¸ªè§†é¢‘ã€‚è®²å¾—éå¸¸æ¸…æ¥šã€‚</p>

<p><a href="https://www.youtube.com/watch?v=MswxJw-8PvE">https://www.youtube.com/watch?v=MswxJw-8PvE</a></p>

<p>å‚è€ƒï¼š
<a href="https://pytorch.org/docs/stable/autograd.html#in-place-operations-on-tensors">https://pytorch.org/docs/stable/autograd.html#in-place-operations-on-tensors</a>
<a href="https://pytorch.org/docs/stable/notes/extending.html">https://pytorch.org/docs/stable/notes/extending.html</a>
<a href="https://www.youtube.com/watch?v=MswxJw-8PvE">https://www.youtube.com/watch?v=MswxJw-8PvE</a></p>
:ET